# -*- coding: utf-8 -*-
"""CNN_based_SR Implementation

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YnBbsWlc7-ftwXZmO13wrCIgS-YIBdTj
"""

#In this project we are going to implement our CNN based SR model based on the research paper published
#We are going to implement the exact model archtecture as in the published paper

#importing important libraries
from tensorflow.keras.layers import Input, DepthwiseConv2D,Conv2D,BatchNormalization,ReLU,AvgPool2D,Flatten,Dense
from keras.models import Sequential

#definging our SR CNN Block function
#the publication indicates that we gonna need a batchnormalization and a relu  after each convolution block
#From the research based paper the CNN models implementstion is done using a block of DepthwiseConv2D which is immediately followed by
#a BatchNormalization layer of 3*3 configuration,n then the last layer we pass in the ReLU to complete the first block of our sr cnn block
#so instead of having one expensive convolution block we gonna break it into 2 blocks (one will be depthwise and the 2rd will be the 1 by 1
#pointwise)
def sr_cnn_block(val,filters,strides):
  val=DepthwiseConv2D(kernel_size=3,strides=strides,padding='same')(val)
  val=BatchNormalization()(val)
  val=ReLU()(val)

#so basically what depthwise convolution is doing is that it splits the input tensor  into various channels  as well as the kernel
#and for each split channel ,convolves the input with the corresponding filter,in this case is gonna be 2D tensor
#then we gonna finally stack up the split  output tensors back together
#based on the model architecture given on the publication,we need to add a pointwise of 1*1 to the above depthwise block
  val=Conv2D(filters=filters,kernel_size=1,strides=1)(val)
  val=BatchNormalization()(val)
  val=ReLU()(val)
  return val

#building the architecture of the SR CNN model
#from the published paper the model architecture takes the following format for a better implementation
#first we gonna create our input function to be fed in the data,
#second then stack up to it,a 3*3 depthwise followed by a 1*1 pointwise with a ReLU function at the end of it
#third,we gonna stack up a 2rd (3*3 depthwise followed by a 1*1 pointwise with a ReLU function) block with different number of filters,
#fourth, we now gonna run a for loop which takes into account the first input block and the 2rd cnn block and run it twice as per the model
#architecture
#fifth,we then stack on top of these blocks a 3rd  and 4th cnn blocks as defined on the function above

#input function to take in data
input1=Input(shape=(224,224,3)) # defining the input dimension of the data
val=Conv2D(filters=32,kernel_size=2,strides=2,padding='same')(input1)
val=BatchNormalization()(val)
val=ReLU()(val)

#Note , we are working with images and the images are basically 2D (reason for using Conv2D)
#We will be using Dense for fully connection of the neural network

val=sr_cnn_block(val,filters=64,strides=1) #first SR CNN BLOCK with 64 filters

#we gonna be running a for loop immediately after this 2rd block as per the documentation
val=sr_cnn_block(val,filters=128,strides=2) #2rd SR CNN BLOCK with 128 filters
for i in range(2):
  val=sr_cnn_block(val,filters=128,strides=1)

# val=sr_cnn_block(val,filters=64,strides=1)

# val=sr_cnn_block(val,filters=128,strides=2)
# for i in range(2):
#   val=sr_cnn_block(val,filters=128,strides=1)

# val=sr_cnn_block(val,filters=256,strides=2)
# val=sr_cnn_block(val,filters=256,strides=1)

# val=sr_cnn_block(val,filters=512,strides=2)
# val=sr_cnn_block(val,filters=512,strides=1)

val=sr_cnn_block(val,filters=256,strides=2) #3rd SR CNN BLOCK with 256 filters
val=sr_cnn_block(val,filters=256,strides=1)

val=sr_cnn_block(val,filters=512,strides=2) #4th SR CNN BLOCK with 512 filters
val=sr_cnn_block(val,filters=512,strides=1)

#lastly on the SR CNN Model we gonna ensure thats it is fully connected with average pooling
val=AvgPool2D(pool_size=7,strides=1)(val) 
val=Flatten()(val)
output=Dense(units=1000,activation='softmax')(val)

from tensorflow.keras import Model

#creating our model using the input and output data
model=Model(inputs=input1,outputs=output)

model.summary()

#converting the CNN SR model from a functional model to a Sequeantial model 
#this makes the model easy to transfer learn
model_cnn=Sequential()
for layer in model.layers[:-1]:
  model_cnn.add(layer)

type(model_cnn)

#end of the SR CNN Model implementation
model_cnn.add(Dense(1,activation='sigmoid'))

#Using sample dataset to measure the perfomance of our SR CNN model
#Note ,this SR CNN Model will work any any given image dataset

#I could not find the link to the mentioned dataset in the published paper ,so I needed to test the perfomance of the model using a sample
#dataset from kaggle website
#testing the models perfomance on a sample dataset
#using kaggle dataset for detecting tuberclosis link...https://www.kaggle.com/tawsifurrahman/tuberculosis-tb-chest-xray-dataset

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
#importing important libraries
from keras.layers import Input,Lambda,Dense,Flatten
from keras.models import Model
from keras.applications.resnet import ResNet,ResNet152
from keras.applications.densenet import DenseNet,DenseNet201
from keras.applications.densenet import preprocess_input
from keras.applications.resnet import preprocess_input
from keras.preprocessing.image import ImageDataGenerator
from glob import  glob
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from keras.preprocessing.image import ImageDataGenerator as idg
from keras.layers import Dropout
import math
from keras.preprocessing import image
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D,Dropout,AveragePooling2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from sklearn.utils import shuffle
import cv2
import sklearn
import time
from os import listdir
from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPool2D , Flatten
from keras.preprocessing.image import ImageDataGenerator
import numpy as np
import os
import sys
import keras
from keras import layers
from keras import models
from keras import optimizers
# from keras.optimizers import SGD
import glob
import matplotlib.image as mpimg
import imageio as im
from keras import models
from keras.layers import Dropout
from keras.callbacks import ModelCheckpoint

# %matplotlib inline

#!pip install  tensorflow-gpu

import tensorflow as tf
tf.__version__

#

#resize all images
IMAGE_SIZE=[224,224]

#provide the directory to your dataset folder
tb_datadir='/content/drive/MyDrive/TB dataset'

os.listdir(tb_datadir) #gives the data in our directory

tuberculosis= "/content/drive/MyDrive/TB dataset/Tuberculosis"
print("tuberculosis images :\n" ,os.listdir(tuberculosis)[:10])

normal= "/content/drive/MyDrive/TB dataset/Normal data"
print("\nnormal images :\n" ,os.listdir(normal)[:10])

#lets plot a combination of some normal and tuberculosis data
rows= 3
columns= 8
index_picture= 0

fig= plt.gcf()
fig.set_size_inches(columns*2, rows*2)
index_picture+=5

images_of_tuberculosis_data = [os.path.join(tuberculosis, image) for image in os.listdir(tuberculosis)[index_picture-5:index_picture]]
images_of_normal_data = [os.path.join(normal, image) for image in os.listdir(normal)[index_picture-5:index_picture]]

for imgf, image_path in enumerate(images_of_tuberculosis_data+images_of_normal_data):
    sp = plt.subplot(rows, columns, imgf + 1)
    sp.axis('Off') 

    img = mpimg.imread(image_path)
    plt.imshow(img)

plt.show()

# getting the data for training our models
print("training data :")
training_datagen= ImageDataGenerator(rescale=1/255, zoom_range=0.3,  width_shift_range= 0.4, height_shift_range=0.3, shear_range=0.1, 
                                   horizontal_flip=True,  validation_split = 0.2)

training_data = training_datagen.flow_from_directory(tb_datadir, 
                                              target_size= (224,224),
                                              class_mode= "binary",
                                              batch_size=32,
                                              subset= "training"
                                              )

# getting the validation or testing data as well
print("\nvalidation data :")
validation_datagen= ImageDataGenerator(rescale= 1/255, validation_split= 0.2)

validation_data= validation_datagen.flow_from_directory(tb_datadir, 
                                              target_size= (224,224),
                                              class_mode= "binary",
                                              batch_size=32,
                                              shuffle= False,
                                              subset= "validation"
                                              )

model_cnn.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

model_cnn.fit(training_data, validation_data=validation_data, epochs=20)

