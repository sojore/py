# -*- coding: utf-8 -*-
"""2rd_digital_classification.ipynb

Automatically generated by Colaboratory.


Question 3B

1. using one hot encoding on mnist **dataset**
"""

#downloading the dataset,and obtaining the training the training and testing dataset
from tensorflow.keras.datasets import mnist
(X_train,y_train),(X_test,y_test)=mnist.load_data()

# Commented out IPython magic to ensure Python compatibility.
#visualizing some images
import matplotlib.pyplot as plt
plt.imshow(X_train[4])
plt.show()
# %matplotlib inline

#in this step we gonna use one_hot_encoding to encode the dataset by using the to_categorical keras tensorflow function
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import tensorflow as tf

tf.keras.utils.to_categorical

y_train_encoded=tf.keras.utils.to_categorical(y_train)
y_test_encoded=tf.keras.utils.to_categorical(y_test)

#lets check the shape of the encoded target variable
y_train_encoded.shape

y_train_encoded[5]

#reshaping the training dataset
import numpy as np
X_train_reshaped=np.reshape(X_train,(60000,784))
X_test_reshaped=np.reshape(X_test,(10000,784))
X_test_reshaped.shape

#calculating the mean and standard deviation of our reshaped data to better improve the perfomance of our NN model
X_mean=np.mean(X_train_reshaped)
X_mean2=np.mean(X_test_reshaped)
X_std=np.std(X_train_reshaped)
X_std2=np.std(X_test_reshaped)
X_train_std=(X_train_reshaped-X_mean)/X_std
X_test_std=(X_test_reshaped-X_mean2)/X_std2

y_test_encoded.shape

#we gonna be building a fully connected dense Neural Network with 3 hiddden layers as in the paper description 
#then we check this models perfomance on the one hot encoded mnist dataset
#then we later gonna compare this models perfomance with another Neural Network model with same  number of hidden layers but this time...
#the model will be tested on a binary encoded dataset

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential([
    Dense(64, activation = 'relu', input_shape = (784,)),
    Dense(32, activation = 'relu'),
    Dense(10, activation = 'softmax')
])
model.compile(
    optimizer = 'sgd',
    loss = 'categorical_crossentropy',
    metrics = ['accuracy']
)

model.summary()

#training the model
model.fit(
    X_train_std,
    y_train_encoded,
    epochs = 4
)

loss1, accuracy1 = model.evaluate(X_test_std, y_test_encoded)
loss2, accuracy2 = model.evaluate(X_train_std, y_train_encoded)
print(f'NN model accuracy on train dataset is {accuracy2} and on the test dataset its accuracy is {accuracy1}')

#so we can conclude that one_hot_encoded model is having a high accuracy


#now on to the binary_encoded model to see if we can attain relatively same accuracy



"""next up we show that using binary encoding with an additional layer we can roughly attain the same accuracy"""

#to help in using binary encoding on the mnist dataset ,we will be using tensorflow quantum to help  convert the data into quantum bits
!pip install tensorflow-quantum
!pip install tensorflow==2.4.1

#just to update the packages to align with other packages
import importlib, pkg_resources
importlib.reload(pkg_resources)

#importing important tools to help in building the 2rd model
import tensorflow as tf
import tensorflow_quantum as tfq

import cirq
import sympy
import numpy as np
import seaborn as sns
import collections
from cirq.contrib.svg import SVGCircuit

(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()

#next we need to rescale these images to a range of just (0-1) from (0,255)
X_train, X_test = X_train[..., np.newaxis]/255.0, X_test[..., np.newaxis]/255.0

X_test.shape

#since the mnist dataset has several classes, we just gonna filter them to remain with only 3s,6s for analysis
def filter_3s_and_6s(x, y):
    keep = (y == 3) | (y == 6)
    x, y = x[keep], y[keep]
    y = y == 3
    return x,y



X_train, y_train = filter_3s_and_6s(X_train, y_train)
X_test, y_test = filter_3s_and_6s(X_test, y_test)

X_train.shape

#lets print out some images to see if they have been scaled in the range from (0--1)
plt.imshow(X_train[7, :, :, 0])
plt.colorbar()

#we now gonna downsize these images further to 4*4 to make it easy for binary encoding
X_train_downsized = tf.image.resize(X_train, (4,4)).numpy()
X_test_downsized = tf.image.resize(X_test, (4,4)).numpy()

#writting a function to filter the dataset to remove images that are labeled as belonging to both classes
def remove_contradicting_values(xs, ys):
    mapping = collections.defaultdict(set)
    orig_x = {}
    for x,y in zip(xs,ys):
       orig_x[tuple(x.flatten())] = x
       mapping[tuple(x.flatten())].add(y)
    
    new_x = []
    new_y = []
    for flatten_x in mapping:
      x = orig_x[flatten_x]
      labels = mapping[flatten_x]
      if len(labels) == 1:
          new_x.append(x)
          new_y.append(next(iter(labels)))
      else:
          pass
    
    num_uniq_3 = sum(1 for value in mapping.values() if len(value) == 1 and True in value)
    num_uniq_6 = sum(1 for value in mapping.values() if len(value) == 1 and False in value)
    num_uniq_both = sum(1 for value in mapping.values() if len(value) == 2)

    return np.array(new_x), np.array(new_y)

X_train_new, y_train_new = remove_contradicting_values(X_train_downsized, y_train)
THRESHOLD = 0.5

X_train_encoded = np.array(X_train_new > THRESHOLD, dtype=np.float32)
X_test_encoded = np.array(X_test_downsized > THRESHOLD, dtype=np.float32)

_ = remove_contradicting_values(X_train_encoded, y_train_new)

#function for Encoding truncated classical image into quantum datapoints from the mnist dataset
def convert_to_binary_c(image):
    values = np.ndarray.flatten(image)
    binary_bits = cirq.GridQubit.rect(4, 4)
    binary_c = cirq.Circuit()
    for i, value in enumerate(values):
        if value:
            binary_c.append(cirq.X(binary_bits[i]))
    return binary_c

X_train_binary_c = [convert_to_binary_c(x) for x in X_train_encoded]
X_test_binary_c = [convert_to_binary_c(x) for x in X_test_encoded]

#creating a fully connected dense Neural Network model
def binary_encoded_neural_network_model():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Flatten(input_shape=(4,4,1)))
    model.add(tf.keras.layers.Dense(32, activation='relu'))
    model.add(tf.keras.layers.Dense(16, activation='softmax'))###addiing additional layer
    model.add(tf.keras.layers.Dense(1))
    return model


model = binary_encoded_neural_network_model()
model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(),
              metrics=['accuracy'])

model.fit(X_train_encoded,
          y_train_new,
          batch_size=128,
          epochs=20,
          verbose=2,
          validation_data=(X_test_encoded, y_test))

model.evaluate(X_test_encoded, y_test)



"""Question 3C"""

#creating a fully connected dense Neural Network model
def binary_encoded_neural_network_model():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Flatten(input_shape=(4,4,1)))
    model.add(tf.keras.layers.Dense(4, activation='softmax'))
    model.add(tf.keras.layers.Dense(1))
    return model


model = binary_encoded_neural_network_model()
model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(),
              metrics=['accuracy'])

model.fit(X_train_encoded,
          y_train_new,
          batch_size=128,
          epochs=20,
          verbose=2,
          validation_data=(X_test_encoded, y_test))

model.evaluate(X_test_encoded, y_test)

#you gonna notice that training the binary_encoded_model directly without the additional layer results to a less efficient model 
#with a lower accuracy as shown above

"""END OF THE ONE-HOT-ENCODING AND BINARY-ENCODING NEURAL NETWORK IMPLEMENTATION OF  AND TESTING.  THANK YOU!!!"""