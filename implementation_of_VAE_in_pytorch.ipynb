{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "implementation of VAE in pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "stPmhQpdD7dr"
      },
      "outputs": [],
      "source": [
        "#importing the important libraries\n",
        "from torchvision import datasets, transforms\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision.utils import save_image\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#what GPUs are available\n",
        "dp = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "dp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCawzTPVHfo3",
        "outputId": "3d825581-ee28-48f1-99c4-e3b13cd06ce0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#implementing the VAE \n",
        "class VariationalAutoEncoder(nn.Module):\n",
        "    def __init__(self, number_of_samples=1, imagefeaturedimension=32*20*20, Zdim=20):\n",
        "        super(VariationalAutoEncoder, self).__init__()\n",
        "\n",
        "        #onvolutional layers with full-connected layers for the encoder\n",
        "        self.encConv1 = nn.Conv2d(number_of_samples, 16, 5)\n",
        "        self.encConv2 = nn.Conv2d(16, 32, 5)\n",
        "        self.encFC1 = nn.Linear(imagefeaturedimension, Zdim)\n",
        "        self.encFC2 = nn.Linear(imagefeaturedimension, Zdim)\n",
        "\n",
        "        # fully-connected layer with onvolutional layers for decoder\n",
        "        self.decFC1 = nn.Linear(Zdim, imagefeaturedimension)\n",
        "        self.decConv1 = nn.ConvTranspose2d(32, 16, 5)\n",
        "        self.decConv2 = nn.ConvTranspose2d(16, number_of_samples, 5)\n",
        "\n",
        "    #encoder method\n",
        "    def encoder(self, x_val):\n",
        "        x_val = F.relu(self.encConv1(x_val))\n",
        "        x_val = F.relu(self.encConv2(x_val))\n",
        "        x_val = x_val.view(-1, 32*20*20)\n",
        "        mu = self.encFC1(x_val)\n",
        "        logVariation = self.encFC2(x_val)\n",
        "        return mu, logVariation\n",
        "\n",
        "    #Reparameterization method\n",
        "    def reparameterize(self, mu, logVariation):\n",
        "        std = torch.exp(logVariation/2)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + std * eps\n",
        "    #decoder method\n",
        "    def decoder(self, z):\n",
        "        x_val = F.relu(self.decFC1(z))\n",
        "        x_val = x_val.view(-1, 32, 20, 20)\n",
        "        x_val = F.relu(self.decConv1(x_val))\n",
        "        x_val = torch.sigmoid(self.decConv2(x_val))\n",
        "        return x_val\n",
        "    #feed forward method\n",
        "    def forward(self, x):\n",
        "        mu, logVariation = self.encoder(x)\n",
        "        z = self.reparameterize(mu, logVariation)\n",
        "        out = self.decoder(z)\n",
        "        return out, mu, logVariation"
      ],
      "metadata": {
        "id": "aEHN3DcuEKEy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training procedure implementation\n",
        "#defining Hyperparameters\n",
        "learning_rate = 1e-3\n",
        "num_epochs = 10  #change the no of epochs to 80 and compare results as well\n",
        "batch_size = 128\n",
        "\n",
        "\n",
        "#Creating  dataloaders to feed data to the NN\n",
        "training_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=True, download=True,\n",
        "                    transform=transforms.ToTensor()),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "testing_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
        "    batch_size=1)\n",
        "\n",
        "\n",
        "\n",
        "#defining the network with the Adam optimizer\n",
        "\n",
        "net = VariationalAutoEncoder().to(dp)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "#training the network\n",
        "for epoch in range(num_epochs):\n",
        "    for idx, data in enumerate(training_loader, 0):\n",
        "        images, _ = data\n",
        "        images = images.to(dp)\n",
        "\n",
        "        # Feeding a batch of images into the network to obtain the output image, mu, and logVar\n",
        "        out, mu, logVar = net(images)\n",
        "\n",
        "        # The loss is the BCE loss combined with the KL divergence to ensure the distribution is learnt\n",
        "        kl_divergence = 0.5 * torch.sum(-1 - logVar + mu.pow(2) + logVar.exp())\n",
        "        loss = F.binary_cross_entropy(out, images, size_average=False) + kl_divergence\n",
        "\n",
        "        # Backpropagation based on the loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print('Epoch {}: Loss {}'.format(epoch, loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uv0ymog9Ej3V",
        "outputId": "4fab7810-fa6d-4793-dd71-0457aa2816b5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss 11148.2880859375\n",
            "Epoch 1: Loss 10158.416015625\n",
            "Epoch 2: Loss 10007.6220703125\n",
            "Epoch 3: Loss 10543.0166015625\n",
            "Epoch 4: Loss 9802.322265625\n",
            "Epoch 5: Loss 10318.798828125\n",
            "Epoch 6: Loss 9935.015625\n",
            "Epoch 7: Loss 9542.548828125\n",
            "Epoch 8: Loss 9277.42578125\n",
            "Epoch 9: Loss 9526.189453125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#visualizing the data to make comparisons (ploting both original and new image to monitor the changes)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    for data in random.sample(list(testing_loader), 1):\n",
        "        images, _ = data\n",
        "        images = images.to(dp)\n",
        "        img = np.transpose(images[0].cpu().numpy(), [1,2,0])\n",
        "        plt.subplot(121)\n",
        "        plt.imshow(np.squeeze(img))\n",
        "        out, mu, logVariation = net(images)\n",
        "        outimg = np.transpose(out[0].cpu().numpy(), [1,2,0])\n",
        "        plt.subplot(122)\n",
        "        plt.imshow(np.squeeze(outimg))\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "cVLNuXNvE1YD",
        "outputId": "c2a061d9-6a7e-41d6-cd3c-b6ff791b4476"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "END OF IMPLEMENTATION AND TESTING!!! THANK YOU!!!"
      ],
      "metadata": {
        "id": "9ivTYhOoPHDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2Uc6aS-2PPy_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}